from __future__ import annotations

import re
import multiprocessing as mp
import queue
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any
from urllib.parse import urlencode

import streamlit as st

from stock_screener.pipelines.daily_batch import DailyBatchPipeline
from stock_screener.storage.db import init_db
from stock_screener.storage.repository import Repository

DB_PATH = Path("data/screener.db")
DB_PATH.parent.mkdir(parents=True, exist_ok=True)
init_db(DB_PATH)
repo = Repository(DB_PATH)
pipeline = DailyBatchPipeline(DB_PATH)

st.set_page_config(layout="wide", page_title="KR Fundamental Screener")
st.title("ğŸ‡°ğŸ‡· í•œêµ­ ì£¼ì‹ Fundamental Screener (pykrx + SQLite cache)")
st.caption("ìµœì´ˆ ì‹¤í–‰ ì‹œ pykrx ìˆ˜ì§‘ìœ¼ë¡œ ì‹œê°„ì´ ê±¸ë¦¬ë©°, ì´í›„ì—ëŠ” DB snapshotì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.")


@dataclass(frozen=True)
class FilterSpec:
    name: str
    ftype: str
    default: Any


FILTER_SPECS: list[FilterSpec] = [
    FilterSpec("ticker_input", "str", ""),
    FilterSpec("mkt", "list", []),
    FilterSpec("apply_mcap_min", "bool", False),
    FilterSpec("mcap_min", "float", 0.0),
    FilterSpec("apply_value_min", "bool", False),
    FilterSpec("value_min", "float", 0.0),
    FilterSpec("apply_pbr_max", "bool", False),
    FilterSpec("pbr_max", "float", 1.0),
    FilterSpec("apply_roe_min", "bool", False),
    FilterSpec("roe_min", "float", 0.1),
    FilterSpec("apply_eps_positive", "bool", False),
    FilterSpec("apply_reserve_ratio_min", "bool", False),
    FilterSpec("reserve_ratio_min", "float", 500.0),
    FilterSpec("apply_eps_cagr_5y", "bool", False),
    FilterSpec("eps_cagr_5y_min", "float", 0.15),
    FilterSpec("apply_eps_yoy_q", "bool", False),
    FilterSpec("eps_yoy_q_min", "float", 0.25),
    FilterSpec("above_200ma", "bool", False),
    FilterSpec("apply_near_high", "bool", False),
    FilterSpec("near_high_min", "float", 0.9),
    FilterSpec("sort_col", "str", "mcap"),
    FilterSpec("ascending", "bool", False),
    FilterSpec("limit", "int", 100),
]


def _get_query_params() -> dict[str, Any]:
    if hasattr(st, "query_params"):
        return dict(st.query_params)
    return st.experimental_get_query_params()


def _set_query_params(params: dict[str, Any]) -> None:
    if hasattr(st, "query_params"):
        qp = st.query_params
        qp.clear()
        for key, value in params.items():
            qp[key] = value
        return
    st.experimental_set_query_params(**params)


def _safe_rerun() -> None:
    if hasattr(st, "rerun"):
        st.rerun()
        return
    st.experimental_rerun()


def _parse_bool(raw: Any, *, default: bool) -> bool:
    if raw is None:
        return default
    value = raw[0] if isinstance(raw, list) and raw else raw
    normalized = str(value).strip().lower()
    if normalized in {"1", "true", "t", "yes", "y", "on"}:
        return True
    if normalized in {"0", "false", "f", "no", "n", "off"}:
        return False
    raise ValueError(f"invalid bool: {value}")


def _parse_num(raw: Any, cast_type: type[int] | type[float], *, default: int | float) -> int | float:
    if raw is None:
        return default
    value = raw[0] if isinstance(raw, list) and raw else raw
    try:
        return cast_type(value)
    except (TypeError, ValueError) as exc:
        raise ValueError(f"invalid {cast_type.__name__}: {value}") from exc


def _parse_list(raw: Any, *, default: list[str]) -> list[str]:
    if raw is None:
        return default
    values = raw if isinstance(raw, list) else str(raw).split(",")
    return [item.strip() for item in values if str(item).strip()]


def _parse_str(raw: Any, *, default: str) -> str:
    if raw is None:
        return default
    if isinstance(raw, list):
        return str(raw[0]) if raw else default
    return str(raw)


def _parse_query_filter_value(spec: FilterSpec, query_params: dict[str, Any]) -> Any:
    raw = query_params.get(spec.name)
    if spec.ftype == "bool":
        return _parse_bool(raw, default=spec.default)
    if spec.ftype == "int":
        return _parse_num(raw, int, default=spec.default)
    if spec.ftype == "float":
        return _parse_num(raw, float, default=spec.default)
    if spec.ftype == "list":
        return _parse_list(raw, default=list(spec.default))
    return _parse_str(raw, default=spec.default)


def _serialize_query_filter_value(spec: FilterSpec, value: Any) -> str | list[str] | None:
    if value == spec.default or value in (None, ""):
        return None
    if spec.ftype == "bool":
        return "1" if bool(value) else "0"
    if spec.ftype in {"int", "float", "str"}:
        return str(value)
    if spec.ftype == "list":
        values = [str(item).strip() for item in value if str(item).strip()]
        return values if values else None
    return None


def _job_worker(db_path: str, job_type: str, asof_date: str | None, result_queue: mp.Queue) -> None:
    worker_pipeline = DailyBatchPipeline(Path(db_path))
    try:
        if job_type == "full_refresh":
            result = worker_pipeline.run(asof_date=None)
            result_queue.put({"status": "success", "job_type": job_type, "result": result.__dict__})
            return

        if job_type in {"snapshot_refresh", "auto_snapshot_sync"}:
            result = worker_pipeline.rebuild_snapshot_only(asof_date=asof_date)
            result_queue.put({"status": "success", "job_type": job_type, "result": result.__dict__})
            return

        if job_type == "reserve_refresh":
            updated_asof, updated_rows = worker_pipeline.update_reserve_ratio_only(asof_date=asof_date)
            snap_result = worker_pipeline.rebuild_snapshot_only(asof_date=updated_asof)
            result_queue.put(
                {
                    "status": "success",
                    "job_type": job_type,
                    "updated_asof": updated_asof,
                    "updated_rows": updated_rows,
                    "snapshot_rows": snap_result.snapshot,
                }
            )
            return

        result_queue.put({"status": "error", "job_type": job_type, "error": f"Unknown job type: {job_type}"})
    except Exception as exc:  # noqa: BLE001
        result_queue.put({"status": "error", "job_type": job_type, "error": str(exc)})


def _start_background_job(job_type: str, label: str, asof_date: str | None) -> None:
    active_job = st.session_state.get("active_job")
    if active_job and active_job["process"].is_alive():
        st.warning("ë‹¤ë¥¸ ì‘ì—…ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ì™„ë£Œë˜ê±°ë‚˜ ì·¨ì†Œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
        return

    result_queue: mp.Queue = mp.Queue()
    process = mp.Process(target=_job_worker, args=(str(DB_PATH), job_type, asof_date, result_queue), daemon=True)
    process.start()
    st.session_state.active_job = {
        "job_type": job_type,
        "label": label,
        "asof_date": asof_date,
        "process": process,
        "result_queue": result_queue,
        "started_at": time.time(),
    }


def _poll_background_job() -> None:
    active_job = st.session_state.get("active_job")
    if not active_job:
        return

    process = active_job["process"]
    result_queue = active_job["result_queue"]
    if process.is_alive():
        return

    process.join(timeout=0.2)
    message: dict[str, Any]
    try:
        message = result_queue.get_nowait()
    except queue.Empty:
        message = {"status": "error", "job_type": active_job["job_type"], "error": "ì‘ì—… ê²°ê³¼ë¥¼ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}

    st.session_state.last_job_message = message
    st.session_state.active_job = None

    if message.get("status") == "success":
        if message.get("job_type") in {"full_refresh", "snapshot_refresh", "auto_snapshot_sync"}:
            result = message.get("result", {})
            st.session_state.asof = result.get("asof_date")
            if message.get("job_type") == "auto_snapshot_sync" and result.get("asof_date"):
                st.session_state.auto_snapshot_synced_for = result["asof_date"]
        elif message.get("job_type") == "reserve_refresh":
            st.session_state.asof = message.get("updated_asof")


def _render_active_job_panel() -> None:
    active_job = st.session_state.get("active_job")
    if not active_job:
        return

    elapsed = int(time.time() - active_job["started_at"])
    st.info(f"{active_job['label']} ì‹¤í–‰ ì¤‘... ({elapsed}ì´ˆ ê²½ê³¼)")
    c1, c2 = st.columns([1, 1])
    with c1:
        if st.button("ì§„í–‰ìƒíƒœ ìƒˆë¡œê³ ì¹¨", key="refresh_active_job"):
            _safe_rerun()
    with c2:
        if st.button("ì‘ì—… ì·¨ì†Œ", type="secondary", key="cancel_active_job"):
            process = active_job["process"]
            if process.is_alive():
                process.terminate()
                process.join(timeout=1)
            st.session_state.active_job = None
            st.session_state.last_job_message = {
                "status": "cancelled",
                "job_type": active_job["job_type"],
                "message": f"{active_job['label']} ì‘ì—…ì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.",
            }
            _safe_rerun()


query_params = _get_query_params()
if "query_params_restored" not in st.session_state:
    st.session_state.query_parse_errors = []
    for spec in FILTER_SPECS:
        try:
            st.session_state[spec.name] = _parse_query_filter_value(spec, query_params)
        except ValueError:
            st.session_state[spec.name] = spec.default
            st.session_state.query_parse_errors.append(spec.name)
    st.session_state.query_params_restored = True

if st.session_state.get("query_parse_errors"):
    st.warning(
        "ì¼ë¶€ URL í•„í„°ê°’ì„ ë³µì›í•˜ì§€ ëª»í•´ ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤: "
        + ", ".join(st.session_state.query_parse_errors)
    )

_poll_background_job()

last_job_message = st.session_state.pop("last_job_message", None)
if last_job_message:
    status = last_job_message.get("status")
    job_type = last_job_message.get("job_type")
    if status == "success":
        if job_type == "full_refresh":
            result = last_job_message.get("result", {})
            st.success(
                f"ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {result.get('asof_date')} | í‹°ì»¤ {result.get('tickers', 0)}ê°œ | "
                f"prices {result.get('prices', 0):,}ê±´ | cap {result.get('cap', 0):,}ê±´ | "
                f"fundamental {result.get('fundamental', 0):,}ê±´ | snapshot {result.get('snapshot', 0):,}ê±´"
            )
        elif job_type in {"snapshot_refresh", "auto_snapshot_sync"}:
            result = last_job_message.get("result", {})
            if job_type == "auto_snapshot_sync":
                st.success(f"ìµœì‹  ê±°ë˜ì¼ snapshot ìë™ ë™ê¸°í™” ì™„ë£Œ: {result.get('asof_date')}")
            else:
                st.success(f"ìŠ¤ëƒ…ìƒ· ì¬ê³„ì‚° ì™„ë£Œ: {result.get('asof_date')} | snapshot {result.get('snapshot', 0):,}ê±´")
        elif job_type == "reserve_refresh":
            st.success(
                f"ìœ ë³´ìœ¨ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {last_job_message.get('updated_asof')} | "
                f"reserve_ratio {last_job_message.get('updated_rows', 0):,}ê±´ | "
                f"snapshot {last_job_message.get('snapshot_rows', 0):,}ê±´"
            )
    elif status == "cancelled":
        st.warning(last_job_message.get("message", "ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."))
    else:
        st.error(f"ì‘ì—… ì‹¤íŒ¨({job_type}): {last_job_message.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")

if "asof" not in st.session_state:
    st.session_state.asof = repo.get_latest_price_date() or repo.get_latest_snapshot_date()

latest_price_date = repo.get_latest_price_date()
latest_snapshot_date = repo.get_latest_snapshot_date()
if latest_price_date and latest_price_date != latest_snapshot_date:
    auto_sync_target = latest_price_date
    active_job = st.session_state.get("active_job")
    if st.session_state.get("auto_snapshot_synced_for") != auto_sync_target and not active_job:
        _start_background_job("auto_snapshot_sync", "ìµœì‹  ê±°ë˜ì¼ snapshot ìë™ ë™ê¸°í™”", auto_sync_target)
        _safe_rerun()

_render_active_job_panel()

c1, c2, c3 = st.columns([1, 1, 1])
with c1:
    refresh_full = st.button("ì „ì²´ ìˆ˜ì§‘ + ìŠ¤ëƒ…ìƒ·", type="primary")
with c2:
    refresh_snapshot = st.button("ìŠ¤ëƒ…ìƒ·ë§Œ ì¬ê³„ì‚°", help="ì´ë¯¸ ìˆ˜ì§‘ëœ DB ë°ì´í„°ë¡œ snapshotë§Œ ë‹¤ì‹œ ê³„ì‚°")
with c3:
    refresh_reserve = st.button("ìœ ë³´ìœ¨ë§Œ ì—…ë°ì´íŠ¸", help="ë„¤ì´ë²„ í¬ë¡¤ë§ìœ¼ë¡œ ìµœì‹  ìœ ë³´ìœ¨ë§Œ ì—…ë°ì´íŠ¸")

if refresh_full:
    _start_background_job("full_refresh", "ì „ì²´ ìˆ˜ì§‘ + ìŠ¤ëƒ…ìƒ·", None)
    _safe_rerun()

if refresh_snapshot:
    _start_background_job("snapshot_refresh", "ìŠ¤ëƒ…ìƒ· ì¬ê³„ì‚°", repo.get_latest_price_date())
    _safe_rerun()

if refresh_reserve:
    _start_background_job("reserve_refresh", "ìœ ë³´ìœ¨ ì—…ë°ì´íŠ¸ + ìŠ¤ëƒ…ìƒ· ì¬ê³„ì‚°", repo.get_latest_price_date())
    _safe_rerun()

asof = st.session_state.asof
if not asof:
    st.warning("snapshotì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € 'ì „ì²´ ìˆ˜ì§‘ + ìŠ¤ëƒ…ìƒ·' ë˜ëŠ” 'ìŠ¤ëƒ…ìƒ·ë§Œ ì¬ê³„ì‚°' ë²„íŠ¼ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    st.stop()

base = repo.load_snapshot(asof)
if base.empty:
    st.warning("ì„ íƒí•œ asof_date snapshotì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ìˆ˜ì§‘í•´ ì£¼ì„¸ìš”.")
    st.stop()

st.subheader(f"Snapshot as of {asof}")
st.write(f"í˜„ì¬ snapshot ì¢…ëª© ìˆ˜: **{len(base):,}ê°œ**")

st.markdown("### ì¡°ê±´ ì„ íƒ")
st.caption("ì›í•˜ëŠ” ì¡°ê±´ë§Œ ì²´í¬í•´ì„œ ì ìš©í•˜ì„¸ìš”. ì²´í¬í•˜ì§€ ì•Šì€ ì¡°ê±´ì€ í•„í„°ì— ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

descriptive_tab, fundamental_tab, technical_tab = st.tabs(["Descriptive", "Fundamental", "Technical"])

with descriptive_tab:
    ticker_input = st.text_input("í‹°ì»¤ ì§ì ‘ ì…ë ¥", help="ì½¤ë§ˆ(,) ë˜ëŠ” ê³µë°±ìœ¼ë¡œ ì—¬ëŸ¬ í‹°ì»¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.", key="ticker_input")

    raw_tickers = [token.strip().upper() for token in re.split(r"[\s,]+", ticker_input or "") if token.strip()]
    ticker_list = list(dict.fromkeys(raw_tickers))

    mkt = st.multiselect("ì‹œì¥", sorted(base["market"].dropna().unique().tolist()), key="mkt")

    apply_mcap_min = st.checkbox("ìµœì†Œ ì‹œì´(ì›) ì ìš©", key="apply_mcap_min")
    mcap_min = st.number_input(
        "ìµœì†Œ ì‹œì´(ì›)",
        min_value=0.0,

        step=100_000_000.0,
        disabled=not apply_mcap_min,
        key="mcap_min",
    )

    apply_value_min = st.checkbox("ìµœì†Œ 20D í‰ê·  ê±°ë˜ëŒ€ê¸ˆ(ì›) ì ìš©", key="apply_value_min")
    value_min = st.number_input(
        "ìµœì†Œ 20D í‰ê·  ê±°ë˜ëŒ€ê¸ˆ(ì›)",
        min_value=0.0,

        step=100_000_000.0,
        disabled=not apply_value_min,
        key="value_min",
    )

with fundamental_tab:
    apply_pbr_max = st.checkbox("ìµœëŒ€ PBR ì ìš©", key="apply_pbr_max")
    pbr_max = st.number_input("ìµœëŒ€ PBR", min_value=0.0, step=0.1, disabled=not apply_pbr_max, key="pbr_max")

    apply_roe_min = st.checkbox("ìµœì†Œ ROE proxy ì ìš©", key="apply_roe_min")
    roe_min = st.number_input("ìµœì†Œ ROE proxy", step=0.01, disabled=not apply_roe_min, key="roe_min")

    apply_eps_positive = st.checkbox("EPS í‘ì ê¸°ì—…ë§Œ(ì ì ì œì™¸)", key="apply_eps_positive")

    apply_reserve_ratio_min = st.checkbox("ìµœì†Œ ìœ ë³´ìœ¨(%) ì ìš©", key="apply_reserve_ratio_min")
    reserve_ratio_min = st.number_input(
        "ìµœì†Œ ìœ ë³´ìœ¨(%)", step=50.0, disabled=not apply_reserve_ratio_min, key="reserve_ratio_min"
    )

    apply_eps_cagr_5y = st.checkbox("ìµœê·¼ 5ë…„ EPS CAGR ì¡°ê±´ ì ìš©", key="apply_eps_cagr_5y")
    eps_cagr_5y_min = st.number_input(
        "ìµœê·¼ 5ë…„ EPS CAGR ìµœì†Œ",

        step=0.01,
        format="%.2f",
        disabled=not apply_eps_cagr_5y,
        key="eps_cagr_5y_min",
    )

    apply_eps_yoy_q = st.checkbox("ìµœê·¼ ë¶„ê¸° EPS YoY ì¡°ê±´ ì ìš©", key="apply_eps_yoy_q")
    eps_yoy_q_min = st.number_input(
        "ìµœê·¼ ë¶„ê¸° EPS YoY ìµœì†Œ",

        step=0.01,
        format="%.2f",
        disabled=not apply_eps_yoy_q,
        key="eps_yoy_q_min",
    )

with technical_tab:
    above_200ma = st.checkbox("200ì¼ì„  ìœ„ ì¡°ê±´ ì ìš©", key="above_200ma")

    apply_near_high = st.checkbox("í˜„ì¬ê°€ / 52ì£¼ ì‹ ê³ ê°€ ì¡°ê±´ ì ìš©", key="apply_near_high")
    near_high_min = st.number_input(
        "í˜„ì¬ê°€ / 52ì£¼ ì‹ ê³ ê°€ ìµœì†Œ",

        step=0.01,
        format="%.2f",
        disabled=not apply_near_high,
        key="near_high_min",
    )


active_filter_count = sum(
    [
        int(bool(ticker_list)),
        int(bool(mkt)),
        int(apply_mcap_min),
        int(apply_value_min),
        int(apply_pbr_max),
        int(apply_reserve_ratio_min),
        int(apply_roe_min),
        int(apply_eps_positive),
        int(above_200ma),
        int(apply_eps_cagr_5y),
        int(apply_eps_yoy_q),
        int(apply_near_high),
    ]
)
st.caption(f"ì ìš© ì¤‘ì¸ ì¡°ê±´ ìˆ˜: {active_filter_count}ê°œ")

filtered = base.copy()
missing_tickers: list[str] = []
if ticker_list:
    available_tickers = set(filtered["ticker"].astype(str).str.strip().str.upper())
    missing_tickers = [ticker for ticker in ticker_list if ticker not in available_tickers]
    filtered = filtered[filtered["ticker"].astype(str).str.strip().str.upper().isin(ticker_list)]

if mkt:
    filtered = filtered[filtered["market"].isin(mkt)]
if apply_mcap_min:
    filtered = filtered[filtered["mcap"] >= mcap_min]
if apply_value_min:
    filtered = filtered[filtered["avg_value_20d"] >= value_min]
if apply_pbr_max:
    filtered = filtered[(filtered["pbr"].notna()) & (filtered["pbr"] <= pbr_max)]
if apply_reserve_ratio_min:
    filtered = filtered[(filtered["reserve_ratio"].notna()) & (filtered["reserve_ratio"] >= reserve_ratio_min)]
if apply_roe_min:
    filtered = filtered[(filtered["roe_proxy"].notna()) & (filtered["roe_proxy"] >= roe_min)]
if apply_eps_positive:
    filtered = filtered[filtered["eps_positive"] == 1]
if above_200ma:
    filtered = filtered[filtered["dist_sma200"] >= 0]
if apply_eps_cagr_5y:
    filtered = filtered[(filtered["eps_cagr_5y"].notna()) & (filtered["eps_cagr_5y"] >= eps_cagr_5y_min)]
if apply_eps_yoy_q:
    filtered = filtered[(filtered["eps_yoy_q"].notna()) & (filtered["eps_yoy_q"] >= eps_yoy_q_min)]
if apply_near_high:
    filtered = filtered[(filtered["near_52w_high_ratio"].notna()) & (filtered["near_52w_high_ratio"] >= near_high_min)]

sort_col = st.selectbox(
    "ì •ë ¬ ì»¬ëŸ¼",
    ["mcap", "pbr", "reserve_ratio", "roe_proxy", "ret_3m", "div", "avg_value_20d", "eps_cagr_5y", "eps_yoy_q", "near_52w_high_ratio"],
    key="sort_col",
)
ascending = st.checkbox("ì˜¤ë¦„ì°¨ìˆœ", key="ascending")
limit = st.slider("ì¶œë ¥ ê°œìˆ˜", min_value=10, max_value=500, step=10, key="limit")

query_filter_state: dict[str, Any] = {}
for spec in FILTER_SPECS:
    serialized = _serialize_query_filter_value(spec, st.session_state.get(spec.name, spec.default))
    if serialized is not None:
        query_filter_state[spec.name] = serialized
_set_query_params(query_filter_state)

share_query_string = urlencode(query_filter_state, doseq=True)
share_link = f"?{share_query_string}" if share_query_string else ""
st.caption("í•„í„° ìƒíƒœê°€ URLì— ìë™ ë°˜ì˜ë©ë‹ˆë‹¤. ë§í¬ë¥¼ ë³µì‚¬í•´ ë™ì¼í•œ ì¡°ê±´ì„ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
st.code(share_link or "(ê¸°ë³¸ í•„í„° ìƒíƒœ: ê³µìœ í•  ì¶”ê°€ íŒŒë¼ë¯¸í„° ì—†ìŒ)", language="text")
st.button("ê³µìœ  ë§í¬ ë³µì‚¬", disabled=True, help="ë¸Œë¼ìš°ì € ì£¼ì†Œì°½ URLì„ ë³µì‚¬í•´ ê³µìœ í•˜ì„¸ìš”.")

filtered = filtered.sort_values(sort_col, ascending=ascending).head(limit)

if ticker_list:
    st.caption(f"í‹°ì»¤ ì§ì ‘ ì…ë ¥: {len(ticker_list)}ê°œ ì¤‘ {len(ticker_list) - len(missing_tickers)}ê°œ ë§¤ì¹­")
    if missing_tickers:
        st.warning("snapshotì— ì—†ëŠ” í‹°ì»¤: " + ", ".join(missing_tickers))

if filtered.empty:
    st.warning("ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤. Growth ì¡°ê±´(EPS CAGR/EPS YoY) ì„ê³„ê°’ì„ ë‚®ì¶”ê±°ë‚˜ ì²´í¬ë¥¼ í•´ì œí•´ ë³´ì„¸ìš”.")

show_cols = [
    "ticker", "name", "market", "close", "mcap", "avg_value_20d", "pbr", "reserve_ratio", "per", "div", "dps",
    "eps", "bps", "roe_proxy", "eps_positive", "ret_3m", "ret_1y", "dist_sma200", "pos_52w",
    "near_52w_high_ratio", "eps_cagr_5y", "eps_yoy_q",
]
st.dataframe(filtered[show_cols], width="stretch", hide_index=True)

csv = filtered[show_cols].to_csv(index=False).encode("utf-8-sig")
st.download_button("CSV ë‹¤ìš´ë¡œë“œ", data=csv, file_name=f"screener_{asof}.csv", mime="text/csv")
